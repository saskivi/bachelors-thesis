
@inproceedings{albiolDetectionTVCommercials2004,
  title = {Detection of {{TV}} Commercials},
  booktitle = {2004 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Albiol, A. and Ch, M.J. and Albiol, F.A. and Torres, L.},
  year = {2004},
  month = may,
  volume = {3},
  pages = {iii-541},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2004.1326601},
  abstract = {This paper presents a system that labels TV shots either as commercial or program shots. The system uses two observations: logo presence and shot duration. These observations are modeled using HMMs, and a Viterbi decoder is finally used for shot labeling. The system has been tested on several hours of real video, achieving more than 99\% correct labeling.},
  keywords = {Decoding,Hidden Markov models,Labeling,Law,Legal factors,Monitoring,System testing,TV broadcasting,Video recording,Viterbi algorithm},
  annotation = {92 citations (Semantic Scholar/DOI) [2022-02-07]},
  file = {/Users/saskia/Zotero/storage/DCDLLHYW/Albiol et al. - 2004 - Detection of TV commercials.pdf;/Users/saskia/Zotero/storage/U3LNCRXJ/1326601.html}
}

@article{aminikhanghahiSurveyMethodsTime2017,
  title = {A Survey of Methods for Time Series Change Point Detection},
  author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
  year = {2017},
  month = may,
  journal = {Knowledge and Information Systems},
  volume = {51},
  number = {2},
  pages = {339--367},
  issn = {0219-3116},
  doi = {10.1007/s10115-016-0987-z},
  abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
  langid = {english},
  annotation = {521 citations (Semantic Scholar/DOI) [2022-02-06] 342 citations (Crossref) [2022-02-06]},
  file = {/Users/saskia/Zotero/storage/TQ3FLQFK/Aminikhanghahi and Cook - 2017 - A survey of methods for time series change point d.pdf}
}

@article{barryBayesianAnalysisChange1993,
  title = {A {{Bayesian Analysis}} for {{Change Point Problems}}},
  author = {Barry, Daniel and Hartigan, J. A.},
  year = {1993},
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {421},
  pages = {309--319},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290726},
  abstract = {A sequence of observations undergoes sudden changes at unknown times. We model the process by supposing that there is an underlying sequence of parameters partitioned into contiguous blocks of equal parameter values; the beginning of each block is said to be a change point. Observations are then assumed to be independent in different blocks given the sequence of parameters. In a Bayesian analysis it is necessary to give probability distributions to both the change points and the parameters. We use product partition models (Barry and Hartigan 1992), which assume that the probability of any partition is proportional to a product of prior cohesions, one for each block in the partition, and that given the blocks the parameters in different blocks have independent prior distributions. Given the observations a new product partition model holds, with posterior cohesions for the blocks and new independent block posterior distributions for parameters. The product model thus provides a convenient machinery for allowing the data to weight the partitions likely to hold; inference about particular parameters may then be made by first conditioning on the partition, and then averaging over all partitions. The parameter values may be estimated exactly in O(n3) calculations, or to an adequate approximation by Markov sampling techniques that are O(n) in the number of observations. The Markov sampling computations are thus practicable for long sequences. We compare this model with a number of alternative approaches to fitting change points and parameters when the error distribution is normal, then show that the proposed method is superior to the alternatives in detecting sharp short-lived changes in the parameters.},
  file = {/Users/saskia/Zotero/storage/YIXDEZKR/Barry and Hartigan - 1993 - A Bayesian Analysis for Change Point Problems.pdf}
}

@article{bellmanRoutingProblem1958,
  title = {On a Routing Problem},
  author = {Bellman, Richard},
  year = {1958},
  journal = {Quarterly of Applied Mathematics},
  volume = {16},
  number = {1},
  pages = {87--90},
  issn = {0033-569X, 1552-4485},
  doi = {10.1090/qam/102435},
  abstract = {Given a set of N cities, with every two linked by a road, and the times required to traverse these roads, we wish to determine the path from one given city to another given city which minimizes the travel time. The times are not directly proportional to the distances due to varying quality of roads and varying quantities of traffic.},
  langid = {english},
  annotation = {1846 citations (Semantic Scholar/DOI) [2022-03-10]},
  file = {/Users/saskia/Zotero/storage/SBEG6W55/Bellman - 1958 - On a routing problem.pdf;/Users/saskia/Zotero/storage/JACXK5YT/S0033-569X-1958-0102435-2.html}
}

@article{berraniNonsupervisedApproachRepeated2008,
  title = {A Non-Supervised Approach for Repeated Sequence Detection in {{TV}} Broadcast Streams},
  author = {Berrani, Sid-Ahmed and Manson, Ga{\"e}l and Lechat, Patrick},
  year = {2008},
  month = aug,
  journal = {Signal Processing: Image Communication},
  volume = {23},
  number = {7},
  pages = {525--537},
  issn = {09235965},
  doi = {10.1016/j.image.2008.04.018},
  abstract = {In this paper, a novel method for repeated sequence detection in an audio-visual TV broadcast is proposed. This method is required for TV broadcast macro-segmentation which is at the root of many novel services related to TV broadcast and in particular to the TV-on-Demand service. Repeated sequence detection allows inter-program detection (commercials, jingles, credits, . . .), which allows the segmentation of the TV broadcast and the extraction of useful programs. Our method is completely nonsupervised, that is, it does not require a manually created reference database. It relies on a micro-clustering technique that groups similar audio/visual feature vectors. Clusters are then analyzed and repeated sequences are detected. This method is able to continuously analyze the TV broadcast and to periodically return analysis results. The efficiency and effectiveness of the method have been shown on two real broadcasts of 12 h and 7 days.},
  langid = {english},
  annotation = {47 citations (Semantic Scholar/DOI) [2022-03-09]},
  file = {/Users/saskia/Zotero/storage/VVZ78MP9/Berrani et al. - 2008 - A non-supervised approach for repeated sequence de.pdf}
}

@article{birgeMinimalPenaltiesGaussian2007,
  title = {Minimal {{Penalties}} for {{Gaussian Model Selection}}},
  author = {Birg{\'e}, Lucien and Massart, Pascal},
  year = {2007},
  month = may,
  journal = {Probability Theory and Related Fields},
  volume = {138},
  number = {1-2},
  pages = {33},
  publisher = {{Springer Nature B.V.}},
  address = {{Heidelberg, Netherlands}},
  issn = {01788051},
  doi = {http://dx.doi.org/10.1007/s00440-006-0011-8},
  abstract = {This paper is mainly devoted to a precise analysis of what kind of penalties should be used in order to perform model selection via the minimization of a penalized least-squares type criterion within some general Gaussian framework including the classical ones. As compared to our previous paper on this topic (BirgE and Massart in J. Eur. Math. Soc. 3 , 203-268 (2001)), more elaborate forms of the penalties are given which are shown to be, in some sense, optimal. We indeed provide more precise upper bounds for the risk of the penalized estimators and lower bounds for the penalty terms, showing that the use of smaller penalties may lead to disastrous results. These lower bounds may also be used to design a practical strategy that allows to estimate the penalty from the data when the amount of noise is unknown. We provide an illustration of the method for the problem of estimating a piecewise constant signal in Gaussian noise when neither the number, nor the location of the change points are known. [PUBLICATION ABSTRACT]},
  copyright = {Springer-Verlag 2007},
  langid = {english},
  keywords = {Design,Fines \& penalties,Hilbert space,Mathematical models,Mathematics,Noise,Regression analysis,Studies,Variables},
  file = {/Users/saskia/Zotero/storage/RNRNH9AJ/Birgé and Massart - 2007 - Minimal Penalties for Gaussian Model Selection.pdf}
}

@book{brodskyNonparametricMethodsChange1993,
  title = {Nonparametric {{Methods}} in {{Change Point Problems}}},
  author = {Brodsky, E. and Darkhovsky, B. S.},
  year = {1993},
  month = jan,
  publisher = {{Springer Science \& Business Media}},
  abstract = {The explosive development of information science and technology puts in new problems involving statistical data analysis. These problems result from higher re quirements concerning the reliability of statistical decisions, the accuracy of math ematical models and the quality of control in complex systems. A new aspect of statistical analysis has emerged, closely connected with one of the basic questions of cynergetics: how to "compress" large volumes of experimental data in order to extract the most valuable information from data observed. De tection of large "homogeneous" segments of data enables one to identify "hidden" regularities in an object's behavior, to create mathematical models for each seg ment of homogeneity, to choose an appropriate control, etc. Statistical methods dealing with the detection of changes in the characteristics of random processes can be of great use in all these problems. These methods have accompanied the rapid growth in data beginning from the middle of our century. According to a tradition of more than thirty years, we call this sphere of statistical analysis the "theory of change-point detection. " During the last fifteen years, we have witnessed many exciting developments in the theory of change-point detection. New promising directions of research have emerged, and traditional trends have flourished anew. Despite this, most of the results are widely scattered in the literature and few monographs exist. A real need has arisen for up-to-date books which present an account of important current research trends, one of which is the theory of non parametric change--point detection.},
  googlebooks = {Lu\_XN8KswvwC},
  isbn = {978-0-7923-2122-4},
  langid = {english},
  keywords = {Language Arts \& Disciplines / Library \& Information Science / General,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Science / System Theory,Technology \& Engineering / Electrical}
}

@article{dorcaswambuiPowerPrunedExact2015,
  title = {The {{Power}} of the {{Pruned Exact Linear Time}}({{PELT}}) {{Test}} in {{Multiple Changepoint Detection}}},
  author = {Dorcas Wambui, Gachomo},
  year = {2015},
  journal = {American Journal of Theoretical and Applied Statistics},
  volume = {4},
  number = {6},
  pages = {581},
  issn = {2326-8999},
  doi = {10.11648/j.ajtas.20150406.30},
  langid = {english},
  file = {/Users/saskia/Zotero/storage/SCUE439I/Dorcas Wambui - 2015 - The Power of the Pruned Exact Linear Time(PELT) Te.pdf}
}

@article{einmahlEmpiricalLikelihoodBased2003,
  title = {Empirical Likelihood Based Hypothesis Testing},
  author = {Einmahl, J.H.J. and Mckeague, I.W.},
  year = {2003},
  journal = {Bernoulli},
  volume = {9},
  number = {2},
  pages = {267--290},
  issn = {1350-7265},
  doi = {10.3150/bj/1068128978},
  abstract = {Omnibus tests for various nonparametric hypotheses are developed using the empirical likelihood method. These include tests for symmetry about zero, changes in distribution, independence and exponentiality. The approach is to localize the empirical likelihood using a suitable 'time' variable implicit in the null hypothesis and then form an integral of the log-likelihood ratio statistic. The asymptotic null distributions of these statistics are established. In simulation studies, the proposed statistics are found to have greater power than corresponding Cram\'er-von Mises type statistics. \textcopyright{} 2003 ISI/BS.},
  langid = {english},
  keywords = {Change point,Distribution-free,Exponentiality,Independence,Nonparametric likelihood ratio,Symmetry,Two-sample problem},
  annotation = {101 citations (Semantic Scholar/DOI) [2022-02-06]},
  file = {/Users/saskia/Zotero/storage/FTHD28DB/Einmahl and Mckeague - 2003 - Empirical likelihood based hypothesis testing.pdf;/Users/saskia/Zotero/storage/4PVJHZNX/display.html}
}

@article{ibrahimTVStreamStructuring2011,
  title = {{{TV Stream Structuring}}},
  author = {Ibrahim, Zein Al Abidin and Gros, Patrick},
  year = {2011},
  month = jun,
  journal = {ISRN Signal Processing},
  volume = {2011},
  pages = {1--17},
  issn = {2090-5041, 2090-505X},
  doi = {10.5402/2011/975145},
  abstract = {TV stream structuring consists in detecting precisely the first and the last frames of all the programs and the breaks (commercials, trailers, station identification, bumpers) of a given stream and then in annotating all these segments with metadata. Usually, breaks are broadcasted several times during a stream. Thus, the detection of these repetitions can be considered as a key tool for stream structuring. After the detection stage, a classification method is applied to separate the repetitions in programs and breaks. In their turn, breaks repetitions are then used to classify the segments which appear only once in the stream. Finally, the stream is aligned with an electronic program guide (EPG), in order to annotate the programs. Our experiments have been applied on a 22-day long TV stream, and results show the efficiency of the proposed method in TV stream structuring.},
  langid = {english},
  annotation = {11 citations (Semantic Scholar/DOI) [2022-03-09]},
  file = {/Users/saskia/Zotero/storage/SX396LTY/Ibrahim and Gros - 2011 - TV Stream Structuring.pdf}
}

@book{internetarchiveUnderstandingRobustExploratory1983,
  title = {Understanding Robust and Exploratory Data Analysis},
  year = {1983},
  publisher = {{New York : Wiley}},
  abstract = {xvi, 447 p. : 24 cm; Bibliography: p. 427-429; Includes index},
  collaborator = {{Internet Archive}},
  isbn = {978-0-471-09777-8},
  langid = {english},
  keywords = {Mathematical statistics}
}

@article{jacksonAlgorithmOptimalPartitioning2005,
  title = {An Algorithm for Optimal Partitioning of Data on an Interval},
  author = {Jackson, B. and Scargle, J.D. and Barnes, D. and Arabhi, S. and Alt, A. and Gioumousis, P. and Gwin, E. and Sangtrakulcharoen, P. and Tan, L. and Tsai, Tun Tao},
  year = {2005},
  month = feb,
  journal = {IEEE Signal Processing Letters},
  volume = {12},
  number = {2},
  pages = {105--108},
  issn = {1558-2361},
  doi = {10.1109/LSP.2001.838216},
  abstract = {Many signal processing problems can be solved by maximizing the fitness of a segmented model over all possible partitions of the data interval. This letter describes a simple but powerful algorithm that searches the exponentially large space of partitions of N data points in time O(N/sup 2/). The algorithm is guaranteed to find the exact global optimum, automatically determines the model order (the number of segments), has a convenient real-time mode, can be extended to higher dimensional data spaces, and solves a surprising variety of problems in signal detection and characterization, density estimation, cluster analysis, and classification.},
  keywords = {Algorithm design and analysis,Bayesian methods,Bayesian modeling,cluster analysis,Clustering algorithms,density estimation,histograms,Iterative algorithms,Mathematics,NASA,optimization,Partitioning algorithms,Signal analysis,signal detection,Signal detection,Signal processing algorithms},
  file = {/Users/saskia/Zotero/storage/N8TDQP7F/Jackson et al. - 2005 - An algorithm for optimal partitioning of data on a.pdf;/Users/saskia/Zotero/storage/FDXEG7TA/1381461.html}
}

@article{killickOptimalDetectionChangepoints2012,
  title = {Optimal {{Detection}} of {{Changepoints With}} a {{Linear Computational Cost}}},
  author = {Killick, R. and Fearnhead, P. and Eckley, I. A.},
  year = {2012},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {107},
  number = {500},
  pages = {1590--1598},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2012.737745},
  langid = {english},
  file = {/Users/saskia/Zotero/storage/UVLBE52W/Killick et al. - 2012 - Optimal Detection of Changepoints With a Linear Co.pdf}
}

@book{kompatsiarisTVContentAnalysis2012,
  title = {{{TV Content Analysis}}: {{Techniques}} and {{Applications}}},
  shorttitle = {{{TV Content Analysis}}},
  author = {Kompatsiaris, Yiannis and Merialdo, Bernard and Lian, Shiguo},
  year = {2012},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {The rapid advancement of digital multimedia technologies has not only revolutionized the production and distribution of audiovisual content, but also created the need to efficiently analyze TV programs to enable applications for content managers and consumers. Leaving no stone unturned, TV Content Analysis: Techniques and Applications provides a de},
  googlebooks = {jy\_3DwAAQBAJ},
  isbn = {978-1-4398-5562-1},
  langid = {english},
  keywords = {Computers / Data Science / Data Analytics,Computers / Information Technology,Computers / Internet / General,Computers / Software Development \& Engineering / Computer Graphics}
}

@article{leeApplyRelayRecording2010,
  title = {Apply Relay Recording and Video Segment Annotation for {{IPTV}} Network Personal Video Recorder},
  author = {Lee, Meng-Huang},
  year = {2010},
  month = nov,
  journal = {IEEE Transactions on Consumer Electronics},
  volume = {56},
  number = {4},
  pages = {2364--2372},
  issn = {1558-4127},
  doi = {10.1109/TCE.2010.5681113},
  abstract = {This paper presents a relay recording model for supporting user's recording at random from DTV live broadcast channels for IPTV nPVR services. Based on the recorded files, SMIL annotation is used to playback a recording in a segmentation fashion for users' viewing their recordings. A relay recording algorithm is proposed and its implementation issues are addressed. Some analyses and evaluations show that as compared with the traditional recording model that records channels all the time, the proposed relay recording model is beneficial on the storage usage of nPVR server for most live broadcast channelS.},
  keywords = {Antennas,IPTV,IPTV; SMIL; nPVR; relay recording.,Middleware,Relays,Servers},
  annotation = {2 citations (Semantic Scholar/DOI) [2022-02-07]},
  file = {/Users/saskia/Zotero/storage/WUGHZED6/Lee - 2010 - Apply relay recording and video segment annotation.pdf;/Users/saskia/Zotero/storage/RNGJJNY9/5681113.html}
}

@article{leeNonparametricMultipleChangepoint1996,
  title = {Nonparametric Multiple Change-Point Estimators},
  author = {Lee, Chung-Bow},
  year = {1996},
  month = may,
  journal = {Statistics \& Probability Letters},
  volume = {27},
  number = {4},
  pages = {295--304},
  issn = {0167-7152},
  doi = {10.1016/0167-7152(95)00089-5},
  abstract = {A simple method is proposed to detect the number of change points in a sequence of independent random variables with no distributional assumption. The method is based on the weighted empirical measures over a window of observations and then runs the window over the full extent of the data. We find that the class of estimators based on our method will be consistent a.s. (almost surely) to the true number of change points and the difference between the true location of change points and the estimated location will be of order O(log n) a.s. Three examples are investigated by the proposed method.},
  langid = {english},
  keywords = {Change points,Nonparametric estimation,Weighted empirical distribution},
  annotation = {24 citations (Semantic Scholar/DOI) [2022-02-06]},
  file = {/Users/saskia/Zotero/storage/V3GP7RT5/Lee - 1996 - Nonparametric multiple change-point estimators.pdf;/Users/saskia/Zotero/storage/SIAQPYSU/0167715295000895.html}
}

@article{levy-leducDetectionLocalizationChangepoints2009,
  title = {Detection and Localization of Change-Points in High-Dimensional Network Traffic Data},
  author = {{L{\'e}vy-Leduc}, C{\'e}line and Roueff, Fran{\c c}ois},
  year = {2009},
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {3},
  number = {2},
  pages = {637--662},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS232},
  abstract = {We propose a novel and efficient method, that we shall call TopRank in the following paper, for detecting change-points in high-dimensional data. This issue is of growing concern to the network security community since network anomalies such as Denial of Service (DoS) attacks lead to changes in Internet traffic. Our method consists of a data reduction stage based on record filtering, followed by a nonparametric change-point detection test based on U-statistics. Using this approach, we can address massive data streams and perform anomaly detection and localization on the fly. We show how it applies to some real Internet traffic provided by France-T\'el\'ecom (a French Internet service provider) in the framework of the ANR-RNRT OSCAR project. This approach is very attractive since it benefits from a low computational load and is able to detect and localize several types of network anomalies. We also assess the performance of the TopRank algorithm using synthetic data and compare it with alternative approaches based on random aggregation.},
  keywords = {change-point detection,High-dimensional data,Network anomaly detection,rank tests},
  annotation = {64 citations (Semantic Scholar/DOI) [2022-03-09]},
  file = {/Users/saskia/Zotero/storage/JU68FED2/Lévy-Leduc and Roueff - 2009 - Detection and localization of change-points in hig.pdf;/Users/saskia/Zotero/storage/69PJHE4U/08-AOAS232.html}
}

@article{liuChangepointDetectionMethod2018,
  title = {Change-Point Detection Method for Clinical Decision Support System Rule Monitoring},
  author = {Liu, Siqi and Wright, Adam and Hauskrecht, Milos},
  year = {2018},
  month = sep,
  journal = {Artificial Intelligence in Medicine},
  volume = {91},
  pages = {49--56},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2018.06.003},
  abstract = {A clinical decision support system (CDSS) helps clinicians to manage patients, but malfunctions of its components or other systems on which it depends may affect its intended functions. Monitoring the system and detecting changes in its behavior that may indicate the malfunction can help to avoid any potential costs associated with its improper operation. In this paper, we investigate the problem of detecting changes in the CDSS operation, in particular its monitoring and alerting subsystem, by monitoring its rule firing counts. We aim to screen and detect changes in real-time, that is whenever a new datum (rule firing count) arrives, we want to have a score indicating how likely there is a change in the system. We develop a new method based on Seasonal-Trend decomposition with locally weighted regression (Loess) and likelihood ratio statistics to detect the changes. Experiments on daily rule-firing-count data collected from a real CDSS and known change-points show that our method improves the detection performance when compared with existing change-point detection methods.},
  langid = {english},
  keywords = {Anomaly detection,Change-point detection,Clinical decision support system,Nonstationarity,Time series},
  annotation = {8 citations (Semantic Scholar/DOI) [2022-03-09]},
  file = {/Users/saskia/Zotero/storage/3N9UHJP9/Liu et al. - 2018 - Change-point detection method for clinical decisio.pdf;/Users/saskia/Zotero/storage/XRRK4UNM/S093336571730595X.html}
}

@article{lung-yut-fongDistributedDetectionLocalization2012,
  title = {Distributed Detection/Localization of Change-Points in High-Dimensional Network Traffic Data},
  author = {{Lung-Yut-Fong}, A. and {L{\'e}vy-Leduc}, C. and Capp{\'e}, O.},
  year = {2012},
  month = mar,
  journal = {Statistics and Computing},
  volume = {22},
  number = {2},
  pages = {485--496},
  issn = {1573-1375},
  doi = {10.1007/s11222-011-9240-5},
  abstract = {We propose a novel approach for distributed statistical detection of change-points in high-volume network traffic. We consider more specifically the task of detecting and identifying the targets of Distributed Denial of Service (DDoS) attacks. The proposed algorithm, called DTopRank, performs distributed network anomaly detection by aggregating the partial information gathered in a set of network monitors. In order to address massive data while limiting the communication overhead within the network, the approach combines record filtering at the monitor level and a nonparametric rank test for doubly censored time series at the central decision site. The performance of the DTopRank algorithm is illustrated both on synthetic data as well as from a traffic trace provided by a major Internet service provider.},
  langid = {english},
  annotation = {31 citations (Semantic Scholar/DOI) [2022-03-09]},
  file = {/Users/saskia/Zotero/storage/ZUF8JZ7S/Lung-Yut-Fong et al. - 2012 - Distributed detectionlocalization of change-point.pdf}
}

@article{mansonAutomaticTVBroadcast2010,
  title = {Automatic {{TV Broadcast Structuring}}},
  author = {Manson, Ga{\"e}l and Berrani, Sid-Ahmed},
  year = {2010},
  month = mar,
  journal = {International Journal of Digital Multimedia Broadcasting},
  volume = {2010},
  pages = {e153160},
  publisher = {{Hindawi}},
  issn = {1687-7578},
  doi = {10.1155/2010/153160},
  abstract = {TV broadcast structuring is needed to precisely extract long useful programs. These can be either archived as part of our audio-visual heritage or used to build added-value novel TV services like TVoD or Catch-up-TV. First, the problem of digital TV content structuring is positioned. Related work and existing solutions are deeply and carefully analyzed. This paper presents then DealTV, our fully automatic system. It is based on studying repeated sequences in the TV stream in order to segment it. Segments are then classified using an inductive logic programming-based technique that makes use of the temporal relationships between segments. Metadata are finally used to label and extract programs using simple overlapping-based criteria. Each processing step of DealTV has been separately evaluated in order to carefully analyze its impact on the final results. The system has been proven on a real TV stream to be very effective.},
  langid = {english},
  annotation = {22 citations (Semantic Scholar/DOI) [2022-02-07]},
  file = {/Users/saskia/Zotero/storage/E64ZCQS9/Manson and Berrani - 2010 - Automatic TV Broadcast Structuring.pdf;/Users/saskia/Zotero/storage/KG9TMGN2/153160.html}
}

@article{mansonTVBROADCASTMACROSEGMENTATION,
  title = {{{TV BROADCAST MACRO-SEGMENTATION USING THE REPETITION PROPERTY OF INTER-PROGRAMS}}},
  author = {Manson, Gael and Berrani, Sid-Ahmed},
  pages = {7},
  abstract = {This paper addresses the problem of TV stream macro-segmentation which consists of automatically determining the start and the end of each broadcasted program and interprogram (e.g. commercial, trailer). As programs do not share any common features, this paper focuses in particular on detecting inter-programs. Programs are then extracted as the rest of the stream. More precisely, interprograms are detected by their repetition property. This paper shows in a experimental way that almost all interprograms are broadcasted several times in the stream and they are repeated sufficiently in order to achieve an accurate macro-segmentation. It also presents a repeated sequence technique and a rule-based technique that performs macrosegmentation using detected repeated sequences. Our macro-segmentation solution is completely automatic, it outperforms the metadata macro-segmentation that comes with the TV stream and it requires less than two days of accumulated TV stream. The effectiveness of the approach is shown experimentally on a real continuous 7 days TV broadcast.},
  langid = {english},
  file = {/Users/saskia/Zotero/storage/2BVJ9H78/Manson and Berrani - TV BROADCAST MACRO-SEGMENTATION USING THE REPETITI.pdf}
}

@article{ngoVideoTextDetection2005,
  title = {Video Text Detection and Segmentation for Optical Character Recognition},
  author = {Ngo, Chong-Wah and Chan, Chi-Kwong},
  year = {2005},
  month = mar,
  journal = {Multimedia Systems},
  volume = {10},
  number = {3},
  pages = {261--272},
  issn = {1432-1882},
  doi = {10.1007/s00530-004-0157-0},
  abstract = {In this paper, we present approaches to detecting and segmenting text in videos. The proposed video-text-detection technique is capable of adaptively applying appropriate operators for video frames of different modalities by classifying the background complexities. Effective operators such as the repeated shifting operations are applied for the noise removal of images with high edge density. Meanwhile, a text-enhancement technique is used to highlight the text regions of low-contrast images. A coarse-to-fine projection technique is then employed to extract text lines from video frames. Experimental results indicate that the proposed text-detection approach is superior to the machine-learning-based (such as SVM and neural network), multiresolution-based, and DCT-based approaches in terms of detection and false-alarm rates. Besides text detection, a technique for text segmentation is also proposed based on adaptive thresholding. A commercial OCR package is then used to recognize the segmented foreground text. A satisfactory character-recognition rate is reported in our experiments.},
  langid = {english},
  keywords = {Text recognition,Text segmentation,Video text detection},
  annotation = {44 citations (Semantic Scholar/DOI) [2022-04-19]},
  file = {/Users/saskia/Zotero/storage/TN4D6URL/Ngo and Chan - 2005 - Video text detection and segmentation for optical .pdf}
}

@article{rabinerTutorialHiddenMarkov1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, L.R.},
  year = {1989},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  issn = {1558-2256},
  doi = {10.1109/5.18626},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{$<>$}},
  keywords = {Hidden Markov models,Speech recognition,Tutorial},
  annotation = {9996 citations (Semantic Scholar/DOI) [2022-04-19]},
  file = {/Users/saskia/Zotero/storage/3SEQ3ET6/Rabiner - 1989 - A tutorial on hidden Markov models and selected ap.pdf;/Users/saskia/Zotero/storage/7MFGV3R9/18626.html}
}

@article{reevesReviewComparisonChangepoint2007,
  title = {A {{Review}} and {{Comparison}} of {{Changepoint Detection Techniques}} for {{Climate Data}}},
  author = {Reeves, Jaxk and Chen, Jien and Wang, Xiaolan L. and Lund, Robert and Lu, QiQi},
  year = {2007},
  month = jun,
  journal = {Journal of Applied Meteorology and Climatology},
  volume = {46},
  number = {6},
  pages = {900--915},
  publisher = {{American Meteorological Society}},
  address = {{Boston, United States}},
  issn = {15588424},
  abstract = {This review article enumerates, categorizes, and compares many of the methods that have been proposed to detect undocumented changepoints in climate data series. The methods examined include the standard normal homogeneity (SNH) test, Wilcoxon's nonparametric test, two-phase regression (TPR) procedures, inhomogeneity tests, information criteria procedures, and various variants thereof. All of these methods have been proposed in the climate literature to detect undocumented changepoints, but heretofore there has been little formal comparison of the techniques on either real or simulated climate series. This study seeks to unify the topic, showing clearly the fundamental differences among the assumptions made by each procedure and providing guidelines for which procedures work best in different situations. It is shown that the common trend TPR and Sawa's Bayes criteria procedures seem optimal for most climate time series, whereas the SNH procedure and its nonparametric variant are probably best when trend and periodic effects can be diminished by using homogeneous reference series. Two applications to annual mean temperature series are given. Directions for future research are discussed. [PUBLICATION ABSTRACT]},
  copyright = {Copyright American Meteorological Society Jun 2007},
  langid = {english},
  keywords = {Homogenization,Measurement techniques,Meteorology,Methods,Random variables,Standard deviation,Studies,Trends},
  file = {/Users/saskia/Zotero/storage/CNPEDH33/Reeves et al. - 2007 - A Review and Comparison of Changepoint Detection T.pdf}
}

@article{sadlierAutomaticTVAdvertisement2002,
  title = {Automatic {{TV}} Advertisement Detection from {{MPEG}} Bitstream},
  author = {Sadlier, David A. and Marlow, Sean and O'Connor, Noel and Murphy, Noel},
  year = {2002},
  month = dec,
  journal = {Pattern Recognition},
  series = {Pattern {{Recognition}} in {{Information Systems}}},
  volume = {35},
  number = {12},
  pages = {2719--2726},
  issn = {0031-3203},
  doi = {10.1016/S0031-3203(01)00251-5},
  abstract = {The Centre for Digital Video Processing at Dublin City University conducts concentrated research and development in the area of digital video management. The current stage of development is demonstrated on our Web-based digital video system called F{\'\i}schl\'ar (Proceedings of the Content based Multimedia Information Access, RIAO 2000, Vol. 2, Paris, France, 12\textendash 14 April 2000, p. 1390), which provides for efficient recording, analysing, browsing and viewing of digitally captured television programmes. Advertisement breaks during or between television programmes are typically recognised by a series of `black' video frames simultaneously accompanying a depression in audio volume which separate each advertisement from one another by recurrently occurring before and after each individual advertisement. It is the regular prevalence of these flags that enables automatic differentiation between what is programme and what is a commercial break. This paper reports on the progress made in the development of this idea into an advertisement detector system that automatically detects the commercial breaks from the bitstream of digitally captured television broadcasts.},
  langid = {english},
  keywords = {Black/silent frames,DC coefficients,MPEG-1,Subband scalefactors},
  annotation = {88 citations (Semantic Scholar/DOI) [2022-02-07]},
  file = {/Users/saskia/Zotero/storage/7G5N6JG8/Sadlier et al. - 2002 - Automatic TV advertisement detection from MPEG bit.pdf;/Users/saskia/Zotero/storage/B5L9YZ5Q/S0031320301002515.html}
}

@misc{truongRupturesChangePoint2018,
  title = {Ruptures: Change Point Detection in Python},
  author = {Truong, Charles},
  year = {2018}
}

@article{truongSelectiveReviewOffline2020,
  title = {Selective Review of Offline Change Point Detection Methods},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  year = {2020},
  month = feb,
  journal = {Signal Processing},
  volume = {167},
  pages = {107299},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2019.107299},
  abstract = {This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.},
  langid = {english},
  keywords = {Change point detection,Segmentation,Statistical signal processing},
  annotation = {160 citations (Semantic Scholar/DOI) [2022-02-06] 147 citations (Crossref) [2022-02-06]},
  file = {/Users/saskia/Zotero/storage/2NF2CZDX/Truong et al. - 2020 - Selective review of offline change point detection.pdf;/Users/saskia/Zotero/storage/HNXGW7B6/S0165168419303494.html}
}

@misc{tukeyExploratoryDataAnalysis1977,
  title = {Exploratory Data Analysis},
  author = {Tukey, John W.},
  year = {1977},
  publisher = {{Addison-Wesley}},
  langid = {english},
  keywords = {Behavioural Science,Data Analysis,Elementary Textbook,Empirical Distributions,Heuristic Approach,Order Statistics,Time Series},
  annotation = {Published: Behavioral Science: Quantitative Methods. 7616. Reading, Massachusetts etc.: Addison-Wesley Publishing Company. XVI, 506 p. \$ 23.65 (1977). MSC2010: 62-07 = Data analysis (statistics) (MSC2010) MSC2010: 62-01 = Introductory exposition (textbooks, tutorial papers, etc.) pertaining to statistics MSC2010: 92-01 = Introductory exposition (textbooks, tutorial papers, etc.) pertaining to biology},
  file = {/Users/saskia/Zotero/storage/TCSEKXT7/0409.html}
}

@book{tukeyExploratoryDataAnalysis1977a,
  title = {Exploratory Data Analysis},
  author = {Tukey, John W.},
  year = {1977},
  volume = {2},
  isbn = {0-201-07616-0},
  langid = {english},
  file = {/Users/saskia/Zotero/storage/NLZN3X8S/exploratorydataanalysis_tukey.pdf}
}

@article{verbesseltDetectingTrendSeasonal2010a,
  title = {Detecting Trend and Seasonal Changes in Satellite Image Time Series},
  author = {Verbesselt, Jan and Hyndman, Rob and Newnham, Glenn and Culvenor, Darius},
  year = {2010},
  month = jan,
  journal = {Remote Sensing of Environment},
  volume = {114},
  number = {1},
  pages = {106--115},
  issn = {00344257},
  doi = {10.1016/j.rse.2009.08.014},
  abstract = {A wealth of remotely sensed image time series covering large areas is now available to the earth science community. Change detection methods are often not capable of detecting land cover changes within time series that are heavily influenced by seasonal climatic variations. Detecting change within the trend and seasonal components of time series enables the classification of different types of changes. Changes occurring in the trend component often indicate disturbances (e.g. fires, insect attacks), while changes occurring in the seasonal component indicate phenological changes (e.g. change in land cover type). A generic change detection approach is proposed for time series by detecting and characterizing Breaks For Additive Seasonal and Trend (BFAST). BFAST integrates the decomposition of time series into trend, seasonal, and remainder components with methods for detecting change within time series. BFAST iteratively estimates the time and number of changes, and characterizes change by its magnitude and direction. We tested BFAST by simulating 16-day Normalized Difference Vegetation Index (NDVI) time series with varying amounts of seasonality and noise, and by adding abrupt changes at different times and magnitudes. This revealed that BFAST can robustly detect change with different magnitudes ({$>$}0.1 NDVI) within time series with different noise levels (0.01\textendash 0.07 {$\sigma$}) and seasonal amplitudes (0.1\textendash 0.5 NDVI). Additionally, BFAST was applied to 16-day NDVI Moderate Resolution Imaging Spectroradiometer (MODIS) composites for a forested study area in south eastern Australia. This showed that BFAST is able to detect and characterize spatial and temporal changes in a forested landscape. BFAST is not specific to a particular data type and can be applied to time series without the need to normalize for land cover types, select a reference period, or change trajectory. The method can be integrated within monitoring frameworks and used as an alarm system to flag when and where changes occur.},
  langid = {english},
  annotation = {1122 citations (Semantic Scholar/DOI) [2022-03-09]},
  file = {/Users/saskia/Zotero/storage/29GG7CWV/Verbesselt et al. - 2010 - Detecting trend and seasonal changes in satellite .pdf}
}

@inproceedings{vertFastDetectionMultiple2010,
  title = {Fast Detection of Multiple Change-Points Shared by Many Signals Using Group {{LARS}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vert, Jean-philippe and Bleakley, Kevin},
  year = {2010},
  volume = {23},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/saskia/Zotero/storage/RACZ4CPA/Vert and Bleakley - 2010 - Fast detection of multiple change-points shared by.pdf}
}

@article{wuTextfinderAutomaticSystem1999,
  title = {Textfinder: An Automatic System to Detect and Recognize Text in Images},
  shorttitle = {Textfinder},
  author = {Wu, V. and Manmatha, R. and Riseman, E.M.},
  year = {1999},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {21},
  number = {11},
  pages = {1224--1229},
  issn = {1939-3539},
  doi = {10.1109/34.809116},
  abstract = {A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks. Text is first detected using multiscale texture segmentation and spatial cohesion constraints, then cleaned up and extracted using a histogram-based binarization algorithm. An automatic performance evaluation scheme is also proposed.},
  keywords = {Character recognition,Data mining,Image recognition,Image segmentation,Image storage,Neural networks,Optical character recognition software,Optical filters,Strontium,Text recognition},
  annotation = {455 citations (Semantic Scholar/DOI) [2022-04-19]},
  file = {/Users/saskia/Zotero/storage/JB5D63KL/Wu et al. - 1999 - Textfinder an automatic system to detect and reco.pdf;/Users/saskia/Zotero/storage/UCUWD4HI/metrics.html}
}

@article{yaoEstimatingNumberChangepoints1988,
  title = {Estimating the Number of Change-Points via {{Schwarz}}' Criterion},
  author = {Yao, Yi-Ching},
  year = {1988},
  month = feb,
  journal = {Statistics \& Probability Letters},
  volume = {6},
  number = {3},
  pages = {181--189},
  issn = {0167-7152},
  doi = {10.1016/0167-7152(88)90118-6},
  abstract = {An estimator of the number of change-points in an independent normal sequence is proposed via Schwarz' criterion. Weak consistency of this estimator is established.},
  langid = {english},
  keywords = {change-points,dimension of a model,Schwarz' criterion},
  annotation = {584 citations (Semantic Scholar/DOI) [2022-04-09]},
  file = {/Users/saskia/Zotero/storage/U9ETG7EP/Yao - 1988 - Estimating the number of change-points via Schwarz.pdf;/Users/saskia/Zotero/storage/FKMAD2M2/0167715288901186.html}
}

@article{zouNonparametricMaximumLikelihood2014,
  title = {Nonparametric Maximum Likelihood Approach to Multiple Change-Point Problems},
  author = {Zou, C. and Yin, G. and Feng, L. and Wang, Z.},
  year = {2014},
  journal = {Annals of Statistics},
  volume = {42},
  number = {3},
  pages = {970--1002},
  issn = {0090-5364},
  doi = {10.1214/14-AOS1210},
  abstract = {In multiple change-point problems, different data segments often follow different distributions, for which the changes may occur in the mean, scale or the entire distribution from one segment to another.Without the need to know the number of change-points in advance, we propose a nonparametric maximum likelihood approach to detecting multiple change-points. Our method does not impose any parametric assumption on the underlying distributions of the data sequence, which is thus suitable for detection of any changes in the distributions. The number of change-points is determined by the Bayesian information criterion and the locations of the change-points can be estimated via the dynamic programming algorithm and the use of the intrinsic order structure of the likelihood function. Under some mild conditions, we show that the new method provides consistent estimation with an optimal rate. We also suggest a prescreening procedure to exclude most of the irrelevant points prior to the implementation of the nonparametric likelihood method. Simulation studies show that the proposed method has satisfactory performance of identifying multiple change-points in terms of estimation accuracy and computation time. \textcopyright{} Institute of Mathematical Statistics, 2014.},
  langid = {english},
  keywords = {BIC,Change-point estimation,Cramér-von Mises statistic,Dynamic programming,Empirical distribution function,Goodness-of-fit test},
  annotation = {98 citations (Semantic Scholar/DOI) [2022-02-06]},
  file = {/Users/saskia/Zotero/storage/VGSZXVY6/Zou et al. - 2014 - Nonparametric maximum likelihood approach to multi.pdf;/Users/saskia/Zotero/storage/HVEJLATI/display.html}
}


